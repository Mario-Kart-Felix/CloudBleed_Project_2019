{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import math\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "path_json = \"./data/validated/\"\n",
    "json_files = [pos_json for pos_json in os.listdir(path_json) if pos_json.endswith('.json')]\n",
    "        \n",
    "text_list = []\n",
    "index = 0\n",
    "null_count = 0\n",
    "for file_name in json_files:\n",
    "    path = path_json + file_name\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        if('summary' in data):\n",
    "            summary = (data.get('summary'))\n",
    "            text_list.append(summary)\n",
    "        else:\n",
    "            null_count += 1\n",
    "            print(index)\n",
    "            print(json_files[index])\n",
    "    index += 1\n",
    "print(null_count) #Prints out number of entries with no summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is just for random checking of data\n",
    "'''\n",
    "with open(path_json+json_files[16], 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print('action' in data)\n",
    "'''\n",
    "print(len(text_list))\n",
    "print(text_list[14:18])\n",
    "#json_files[14:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final all unique words in a list of texts\n",
    "def allUniqueWords(docList):\n",
    "    uniq = []\n",
    "    for text in docList:\n",
    "        terms = text.split()\n",
    "        for word in terms:\n",
    "            word = word.lower()\n",
    "            if word not in uniq:\n",
    "                uniq.append(word)\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Term Frequency\n",
    "def computeTF(docList, unique):\n",
    "    data = pd.DataFrame(columns=unique)\n",
    "    for i in range(len(docList)):\n",
    "        data.loc[i] = np.zeros(len(unique))\n",
    "    row = 0\n",
    "    col = 0\n",
    "    for text in docList:\n",
    "        terms = text.split()\n",
    "        for word in terms:\n",
    "            word = word.lower()\n",
    "            data.loc[row, word] += 1 \n",
    "        row += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute TFIDF (Term Frequency - Inverse Document Frequency)\n",
    "def computeTFIDF(TF, unique):\n",
    "    doc_data = pd.DataFrame(columns=unique)\n",
    "    doc_data.loc[0] = np.zeros(len(unique))\n",
    "    #print(doc_data)\n",
    "    N = TF.shape[0]\n",
    "    for i in range(N):\n",
    "        for word in TF.columns:\n",
    "            print(word)\n",
    "            if (TF.loc[i, word] > 0):\n",
    "                doc_data.loc[0, word] += 1\n",
    "    for word in doc_data.columns:\n",
    "        doc_data.loc[0, word] = math.log10(N / (1 + doc_data.loc[0, word]))\n",
    "    for word in TF.columns:\n",
    "        for i in range(N):\n",
    "            TF.loc[i, word] = TF.loc[i, word] * doc_data.loc[0,word]\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the indexes of the N greatest element of a list\n",
    "def Nmaxelements(list1, N): \n",
    "    final_list = [] \n",
    "  \n",
    "    for i in range(0, N):  \n",
    "        max1 = 0\n",
    "        maxi = -1\n",
    "          \n",
    "        for j in range(len(list1)):      \n",
    "            if list1[j] > max1: \n",
    "                max1 = list1[j] \n",
    "                maxi = j \n",
    "        list1.remove(max1); \n",
    "        final_list.append(maxi) \n",
    "    return(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the pronouns in a block of text\n",
    "def allPronoun(text):\n",
    "    pro = []\n",
    "    terms = text.split()\n",
    "    for word in terms:\n",
    "        if(not word.islower):\n",
    "            print(word)\n",
    "            pro.append(word)\n",
    "    return pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds all unique words and term frequency (Warning: Takes forever to run)\n",
    "vcdb_unique = allUniqueWords(text_list)\n",
    "vcdb_TF = computeTF(text_list, vcdb_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create short version of data by random index\n",
    "import random\n",
    "def shortVersionRand(short_size, docList):\n",
    "    short_list = []\n",
    "    rand_nums = random.sample(range(len(docList)), short_size)\n",
    "    for i in range(short_size):\n",
    "        short_list.append(docList[i])\n",
    "    return short_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create short version of data by choosing those with equal or more than N sentences\n",
    "def shortVersionLong(docList, N):\n",
    "    short_list = []\n",
    "    for text in docList:\n",
    "        sentences = text.split(\".\")\n",
    "        if(len(sentences) >= N):\n",
    "            short_list.append(text)\n",
    "    return short_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortRand = shortVersionRand(100, text_list)\n",
    "shortLong = shortVersionLong(text_list, 3)\n",
    "print(len(shortRand))\n",
    "print(shortRand[:5])\n",
    "print(len(shortLong))\n",
    "print(shortLong[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds all unique words and term frequency on short list of random index\n",
    "vcdb_unique_rand = allUniqueWords(shortRand)\n",
    "vcdb_TF_rand = computeTF(shortRand, vcdb_unique_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcdb_unique_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcdb_TF_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = computeTFIDF(vcdb_TF_rand, vcdb_unique_rand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizes tokenization (word2vec) and generates CBOW (Continuous Bag of Words) and Skip Gram model\n",
    "#CURRENTLY NOT WORKING AS INTENDED\n",
    "data = [] \n",
    "f = \" \".join(story for story in text_list)\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "\n",
    "# Create CBOW model\n",
    "\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "# Create skip gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "                                             window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genWordCloud(docList):\n",
    "    all_text = \" \".join(story for story in docList)\n",
    "    wordcloud = WordCloud().generate(all_text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genWordCloud(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below is Analysis Done for the Old Dataset</b>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = pd.read_excel(\"./data/Privacy Rights Clearinghouse Top Breaches.xlsx\")\n",
    "br = pd.read_excel(\"./data/Balloon Race Data Breaches Filtered.xlsx\")\n",
    "#Names columns correctly (Run only once)\n",
    "pr.columns = pr.loc[0,:]\n",
    "pr = pr.loc[1:,:]\n",
    "\n",
    "br.columns = br.loc[0,:]\n",
    "br = br.loc[2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_stories = br.loc[:,'story']\n",
    "pr_stories = pr.loc[:,'Description of incident']\n",
    "stories = []\n",
    "stories = br_stories.tolist() + pr_stories.tolist()\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out uninteresting story (columns with interesting story not labeled 'y')\n",
    "boring = pr.loc[413, 'Description of incident'][:33]\n",
    "boring\n",
    "pr_interesting = []\n",
    "for story in pr_stories:\n",
    "    if(story[:33] != boring):\n",
    "        pr_interesting.append(story)\n",
    "len(pr_interesting)\n",
    "\n",
    "br_interesting = br.loc[br['interesting story'] == 'y'].loc[:,'story']\n",
    "interesting_stories = pr_interesting + br_interesting.tolist()\n",
    "len(interesting_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = allUniqueWords(stories)\n",
    "TF = computeTF(stories, unique)\n",
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_story = computeTFIDF(TF, unique)\n",
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs TFIDF (Warning: Takes forever)\n",
    "unique = allUniqueWords(interesting_stories)\n",
    "TFIDF_interesting = computeTFIDF(computeTF(interesting_stories, unique), unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genWordCloud(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genWordCloud(interesting_stories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
